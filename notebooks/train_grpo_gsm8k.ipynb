{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Small Model for Mathematical Reasoning using Reinforcement Learning (GRPO)\n",
    "\n",
    "In this notebook, we explore the implementation of **GRPO (Generalized Reward Policy Optimization)** to train a small language model, specifically **Qwen2.5-0.5B-Instruct**. The goal is to enhance its capabilities in mathematical reasoning tasks without requiring a massive amount of labeled \"chain-of-thought\" data, but rather by rewarding the correct final outcome and proper formatting.\n",
    "\n",
    "We will utilize the **GSM8K dataset**, a standard benchmark consisting of high quality grade school math problems. The objective is to teach the model to generate a valid chain of thought (reasoning) enclosed in specific XML tags before providing the final answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment Configuration and vLLM Installation\n",
    "\n",
    "The first step involves setting up the necessary libraries. We install `vllm`, which is a high-throughput and memory-efficient library for LLM inference and serving. Although we may disable it during the specific training loop to save VRAM on smaller GPUs, it is a standard dependency for modern RL pipelines.\n",
    "\n",
    "**Important Note:** Installing these libraries involving CUDA kernels often requires a session restart. **After running this cell, please go to 'Runtime' > 'Restart Session' to ensure the drivers are loaded correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip uninstall -y tensorflow\n",
    "!pip install \"numpy<2.0\" \"pandas<2.2.0\" \"scipy<1.13.0\" protobuf==3.20.3\n",
    "!pip install vllm\n",
    "!pip install trl"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining the Prompt Structure and System Instructions\n",
    "\n",
    "In Reinforcement Learning from Human Feedback (RLHF), the model is guided by a **System Prompt**. This prompt establishes the expected behavior and, crucially, the **output format**.\n",
    "\n",
    "We define a system prompt that instructs the model to respond in a specific XML structure with `<reasoning>` and `<answer>` tags."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the prompt and load necessary libraries\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preparation and restructuring of GSM8K dataset\n",
    "We define functions to extract the final answer from the dataset format and load the GSM8K training split."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Functions to extract the answer and load the GSM8K dataset\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"Extract the answer from XML-formatted response.\"\"\"\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    \"\"\"Extract answer from GSM8K format (after ####).\"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# Load and prepare the dataset\n",
    "data = load_dataset('openai/gsm8k', 'main')['train']\n",
    "data = data.map(lambda x: {\n",
    "    'prompt': [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': x['question']}\n",
    "    ],\n",
    "    'answer': extract_hash_answer(x['answer'])\n",
    "})\n",
    "\n",
    "print(f\"Dataset loaded with {len(data)} examples\")\n",
    "print(f\"Example prompt: {data[0]['prompt']}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining Reward Functions\n",
    "\n",
    "We define multiple reward functions that evaluate different aspects of the model's output:\n",
    "\n",
    "- **Correctness**: Whether the final answer matches the ground truth\n",
    "- **Integer format**: Whether the answer is a valid integer\n",
    "- **XML format**: Whether the response follows the expected XML structure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Reward Functions\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward for correct final answer.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward for integer answers.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward for strict XML format compliance.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward for soft XML format compliance.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward for presence of XML tags.\"\"\"\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    for text in contents:\n",
    "        count = 0.0\n",
    "        if \"<reasoning>\" in text:\n",
    "            count += 0.125\n",
    "        if \"</reasoning>\" in text:\n",
    "            count += 0.125\n",
    "        if \"<answer>\" in text:\n",
    "            count += 0.125\n",
    "        if \"</answer>\" in text:\n",
    "            count += 0.125\n",
    "        rewards.append(count)\n",
    "    return rewards\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Hyperparameters and Memory Optimization\n",
    "\n",
    "We configure the `GRPOConfig` with hyperparameters optimized for training on consumer-grade GPUs (e.g., T4).\n",
    "\n",
    "Key configurations:\n",
    "- **Float32 precision**: Prevents gradient scaling issues on T4 architecture\n",
    "- **Conservative learning rate**: 5e-6 to prevent catastrophic forgetting\n",
    "- **Gradient accumulation**: Effective batch size of 16 with batch size 8"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "output_dir = \"outputs/Qwen-0.5B-GRPO\"\n",
    "run_name = \"Qwen-0.5B-GRPO-gsm8k\"\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=10,\n",
    "    bf16=False,\n",
    "    fp16=False,  # Float32 for stability on T4\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_generations=4,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=512,\n",
    "    max_steps=400,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    use_vllm=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Loading Model and Tokenizer\n",
    "\n",
    "We load **Qwen/Qwen2.5-0.5B-Instruct**. This is a small but capable instruction-following model that is well-suited for fine-tuning on mathematical reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Verify GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Launching the GRPO Training Loop\n",
    "\n",
    "We initialize the `GRPOTrainer` with our model, the reward functions, and the training arguments defined above.\n",
    "\n",
    "When `trainer.train()` is executed, the following loop occurs:\n",
    "1. The model generates 4 different attempts for a math question.\n",
    "2. The reward functions score these attempts (did it get the math right? did it use the right format?).\n",
    "3. The model updates its weights to increase the probability of generating the high-scoring answers (correct math + correct XML) and decrease the probability of the low-scoring ones."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize Trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=data,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}